cur device is:  cuda:0
BasicModel(
  (backbone): Sequential(
    (input-conv2d): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (input-bn): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (input-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (input-relu): ReLU(inplace=True)
    (0:inter-2-4-conv2d): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (0:inter-bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (0:inter-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (0:inter-relu): ReLU(inplace=True)
    (1:inter-4-8-conv2d): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1:inter-bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1:inter-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (1:inter-relu): ReLU(inplace=True)
  )
  (conv1): Conv2d(8, 97, kernel_size=(3, 3), stride=(2, 2))
  (classifier): Softmax(dim=1)
  (loss): MSELoss()
)
---------------------------TRIANING----------------------------
epoch:0 trainLoss:0.2931871861219406
TESTING: epoch:0 testLoss:0.30136585235595703
epoch:1 trainLoss:0.2656397968530655
epoch:2 trainLoss:0.2444685623049736
epoch:3 trainLoss:0.2277809903025627
epoch:4 trainLoss:0.21441173553466797
epoch:5 trainLoss:0.20361504703760147
epoch:6 trainLoss:0.1947718933224678
epoch:7 trainLoss:0.18753857165575027
epoch:8 trainLoss:0.18141543120145798
epoch:9 trainLoss:0.17621415853500366
epoch:10 trainLoss:0.17187973111867905
TESTING: epoch:10 testLoss:0.2049785852432251
epoch:11 trainLoss:0.1682390421628952
epoch:12 trainLoss:0.1651991531252861
epoch:13 trainLoss:0.16250746697187424
epoch:14 trainLoss:0.16006039828062057
epoch:15 trainLoss:0.1578555479645729
epoch:16 trainLoss:0.15591324120759964
epoch:17 trainLoss:0.15418797731399536
epoch:18 trainLoss:0.15268031507730484
epoch:19 trainLoss:0.15131110697984695
epoch:20 trainLoss:0.15000425279140472
TESTING: epoch:20 testLoss:0.19157715141773224
epoch:21 trainLoss:0.14878883957862854
epoch:22 trainLoss:0.1476249098777771
epoch:23 trainLoss:0.1464674100279808
epoch:24 trainLoss:0.14530546963214874
epoch:25 trainLoss:0.14414072781801224
epoch:26 trainLoss:0.14300422370433807
epoch:27 trainLoss:0.14189805835485458
epoch:28 trainLoss:0.14081354439258575
epoch:29 trainLoss:0.1397501304745674
epoch:30 trainLoss:0.13870259374380112
TESTING: epoch:30 testLoss:0.19397957623004913
epoch:31 trainLoss:0.13765408843755722
epoch:32 trainLoss:0.13660980015993118
epoch:33 trainLoss:0.1355680227279663
epoch:34 trainLoss:0.13452337309718132
epoch:35 trainLoss:0.1334795095026493
epoch:36 trainLoss:0.13243579491972923
epoch:37 trainLoss:0.13140786439180374
epoch:38 trainLoss:0.13038669526576996
epoch:39 trainLoss:0.12937016412615776
epoch:40 trainLoss:0.1283707208931446
TESTING: epoch:40 testLoss:0.19686129689216614
epoch:41 trainLoss:0.12737693265080452
epoch:42 trainLoss:0.12638817727565765
epoch:43 trainLoss:0.1253989040851593
epoch:44 trainLoss:0.12441666796803474
epoch:45 trainLoss:0.12342900410294533
epoch:46 trainLoss:0.1224578209221363
epoch:47 trainLoss:0.12148435786366463
epoch:48 trainLoss:0.12051095813512802
epoch:49 trainLoss:0.11953246966004372
epoch:50 trainLoss:0.11856609210371971
TESTING: epoch:50 testLoss:0.2007295936346054
epoch:51 trainLoss:0.11759427189826965
epoch:52 trainLoss:0.11663235351443291
epoch:53 trainLoss:0.11565245687961578
epoch:54 trainLoss:0.11468681320548058
epoch:55 trainLoss:0.11371174082159996
epoch:56 trainLoss:0.11274870112538338
epoch:57 trainLoss:0.11178060248494148
epoch:58 trainLoss:0.11081909760832787
epoch:59 trainLoss:0.10986949503421783
epoch:60 trainLoss:0.10891633108258247
TESTING: epoch:60 testLoss:0.2054792046546936
epoch:61 trainLoss:0.10797788202762604
epoch:62 trainLoss:0.10705200955271721
epoch:63 trainLoss:0.10613197088241577
epoch:64 trainLoss:0.10520849376916885
epoch:65 trainLoss:0.10430308803915977
epoch:66 trainLoss:0.10338148474693298
epoch:67 trainLoss:0.10248228907585144
epoch:68 trainLoss:0.10159004852175713
epoch:69 trainLoss:0.10069148242473602
epoch:70 trainLoss:0.09979234263300896
TESTING: epoch:70 testLoss:0.2089708298444748
epoch:71 trainLoss:0.09890193119645119
epoch:72 trainLoss:0.09800948947668076
epoch:73 trainLoss:0.09712597355246544
epoch:74 trainLoss:0.09624859690666199
epoch:75 trainLoss:0.0953887365758419
epoch:76 trainLoss:0.0945315957069397
epoch:77 trainLoss:0.09367897361516953
epoch:78 trainLoss:0.09284354001283646
epoch:79 trainLoss:0.09199582412838936
epoch:80 trainLoss:0.09117447957396507
TESTING: epoch:80 testLoss:0.2114351987838745
epoch:81 trainLoss:0.09035398811101913
epoch:82 trainLoss:0.08954128995537758
epoch:83 trainLoss:0.08874718472361565
epoch:84 trainLoss:0.08795662224292755
epoch:85 trainLoss:0.087177574634552
epoch:86 trainLoss:0.0864146538078785
epoch:87 trainLoss:0.08565574511885643
epoch:88 trainLoss:0.08490505442023277
epoch:89 trainLoss:0.08417441323399544
epoch:90 trainLoss:0.0834396556019783
TESTING: epoch:90 testLoss:0.21375533938407898
epoch:91 trainLoss:0.08271052315831184
epoch:92 trainLoss:0.08199219033122063
epoch:93 trainLoss:0.0812799371778965
epoch:94 trainLoss:0.08057770133018494
epoch:95 trainLoss:0.07988324388861656
epoch:96 trainLoss:0.07919876649975777
epoch:97 trainLoss:0.07852198928594589
epoch:98 trainLoss:0.07784264534711838
epoch:99 trainLoss:0.07717181369662285
