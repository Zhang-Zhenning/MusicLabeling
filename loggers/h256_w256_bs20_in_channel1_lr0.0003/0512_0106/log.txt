cur device is:  cuda:0
BasicModel(
  (backbone): Sequential(
    (input-conv2d): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (input-bn): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (input-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (input-relu): ReLU(inplace=True)
    (0:inter-2-4-conv2d): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (0:inter-bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (0:inter-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (0:inter-relu): ReLU(inplace=True)
    (1:inter-4-8-conv2d): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1:inter-bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1:inter-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (1:inter-relu): ReLU(inplace=True)
  )
  (conv1): Conv2d(8, 97, kernel_size=(3, 3), stride=(2, 2))
  (classifier): Softmax(dim=1)
  (loss): MSELoss()
)
---------------------------TRIANING----------------------------
epoch:0 trainLoss:0.2931871861219406
TESTING: epoch:0 testLoss:0.30136585235595703
epoch:1 trainLoss:0.2656398043036461
epoch:2 trainLoss:0.2444685623049736
epoch:3 trainLoss:0.22778097540140152
epoch:4 trainLoss:0.21441172808408737
epoch:5 trainLoss:0.20361504703760147
epoch:6 trainLoss:0.1947718784213066
epoch:7 trainLoss:0.18753871321678162
epoch:8 trainLoss:0.18141555041074753
epoch:9 trainLoss:0.17621422559022903
epoch:10 trainLoss:0.17187953740358353
TESTING: epoch:10 testLoss:0.20497535169124603
epoch:11 trainLoss:0.16823899000883102
epoch:12 trainLoss:0.1651960164308548
epoch:13 trainLoss:0.1624983325600624
epoch:14 trainLoss:0.16004576534032822
epoch:15 trainLoss:0.15783895552158356
epoch:16 trainLoss:0.15590006858110428
epoch:17 trainLoss:0.15417762845754623
epoch:18 trainLoss:0.1526780128479004
epoch:19 trainLoss:0.15129715204238892
epoch:20 trainLoss:0.14998625218868256
TESTING: epoch:20 testLoss:0.19143332540988922
epoch:21 trainLoss:0.14877121150493622
epoch:22 trainLoss:0.14761385321617126
epoch:23 trainLoss:0.1464502289891243
epoch:24 trainLoss:0.14529042690992355
epoch:25 trainLoss:0.1441396325826645
epoch:26 trainLoss:0.14300645142793655
epoch:27 trainLoss:0.14189603179693222
epoch:28 trainLoss:0.14079885929822922
epoch:29 trainLoss:0.1397378295660019
epoch:30 trainLoss:0.1386910155415535
TESTING: epoch:30 testLoss:0.19344909489154816
epoch:31 trainLoss:0.13766014575958252
epoch:32 trainLoss:0.1366294026374817
epoch:33 trainLoss:0.13561294972896576
epoch:34 trainLoss:0.13459822908043861
epoch:35 trainLoss:0.13358232006430626
epoch:36 trainLoss:0.1325720064342022
epoch:37 trainLoss:0.13155751302838326
epoch:38 trainLoss:0.13054456934332848
epoch:39 trainLoss:0.12954218685626984
epoch:40 trainLoss:0.12852924689650536
TESTING: epoch:40 testLoss:0.19638562202453613
epoch:41 trainLoss:0.1275370568037033
epoch:42 trainLoss:0.12654665112495422
epoch:43 trainLoss:0.12554220110177994
epoch:44 trainLoss:0.12454523518681526
epoch:45 trainLoss:0.12355748564004898
epoch:46 trainLoss:0.1225530095398426
epoch:47 trainLoss:0.12155695259571075
epoch:48 trainLoss:0.12057577818632126
epoch:49 trainLoss:0.11960133165121078
epoch:50 trainLoss:0.11862362548708916
TESTING: epoch:50 testLoss:0.20086781680583954
epoch:51 trainLoss:0.11765372008085251
epoch:52 trainLoss:0.11667708680033684
epoch:53 trainLoss:0.11570168286561966
epoch:54 trainLoss:0.1147487498819828
epoch:55 trainLoss:0.11378967016935349
epoch:56 trainLoss:0.11283236742019653
epoch:57 trainLoss:0.11187263950705528
epoch:58 trainLoss:0.110920749604702
epoch:59 trainLoss:0.1099703423678875
epoch:60 trainLoss:0.10901414975523949
TESTING: epoch:60 testLoss:0.20494045317173004
epoch:61 trainLoss:0.10806237533688545
epoch:62 trainLoss:0.1071188896894455
epoch:63 trainLoss:0.10617255419492722
epoch:64 trainLoss:0.10523397848010063
epoch:65 trainLoss:0.10429875552654266
epoch:66 trainLoss:0.10336356982588768
epoch:67 trainLoss:0.10244230553507805
epoch:68 trainLoss:0.1015351377427578
epoch:69 trainLoss:0.10062847286462784
epoch:70 trainLoss:0.09972169622778893
TESTING: epoch:70 testLoss:0.20778478682041168
epoch:71 trainLoss:0.09881052002310753
epoch:72 trainLoss:0.09791797772049904
epoch:73 trainLoss:0.0970202162861824
epoch:74 trainLoss:0.09612804278731346
epoch:75 trainLoss:0.09526696428656578
epoch:76 trainLoss:0.09440184012055397
epoch:77 trainLoss:0.0935342088341713
epoch:78 trainLoss:0.09268244728446007
epoch:79 trainLoss:0.0918385349214077
epoch:80 trainLoss:0.09101328253746033
TESTING: epoch:80 testLoss:0.2109852135181427
epoch:81 trainLoss:0.09019504860043526
epoch:82 trainLoss:0.08937594667077065
epoch:83 trainLoss:0.08858722448348999
epoch:84 trainLoss:0.08779782429337502
epoch:85 trainLoss:0.0870203897356987
epoch:86 trainLoss:0.08625998347997665
epoch:87 trainLoss:0.08550107479095459
epoch:88 trainLoss:0.08474571630358696
epoch:89 trainLoss:0.0840078704059124
epoch:90 trainLoss:0.08327683061361313
TESTING: epoch:90 testLoss:0.21454095840454102
epoch:91 trainLoss:0.08255728334188461
epoch:92 trainLoss:0.08183581009507179
epoch:93 trainLoss:0.08111794292926788
epoch:94 trainLoss:0.08042338490486145
epoch:95 trainLoss:0.07972483709454536
epoch:96 trainLoss:0.07904283702373505
epoch:97 trainLoss:0.07835963740944862
epoch:98 trainLoss:0.07768044620752335
epoch:99 trainLoss:0.07701550424098969
