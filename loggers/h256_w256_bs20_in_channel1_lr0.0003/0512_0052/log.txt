cur device is:  cuda:0
BasicModel(
  (backbone): Sequential(
    (input-conv2d): Conv2d(1, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (input-bn): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (input-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (input-relu): ReLU(inplace=True)
    (0:inter-2-4-conv2d): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (0:inter-bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (0:inter-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (0:inter-relu): ReLU(inplace=True)
    (1:inter-4-8-conv2d): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1:inter-bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1:inter-maxPool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=(1, 1), dilation=1, ceil_mode=False)
    (1:inter-relu): ReLU(inplace=True)
  )
  (conv1): Conv2d(8, 97, kernel_size=(3, 3), stride=(2, 2))
  (classifier): Softmax(dim=1)
  (loss): MSELoss()
)
---------------------------TRIANING----------------------------
epoch:0 trainLoss:0.2931872010231018
TESTING: epoch:0 testLoss:0.30136585235595703
epoch:1 trainLoss:0.2656397968530655
epoch:2 trainLoss:0.2444685623049736
epoch:3 trainLoss:0.2277809903025627
epoch:4 trainLoss:0.21441172063350677
epoch:5 trainLoss:0.20361504703760147
epoch:6 trainLoss:0.1947718784213066
epoch:7 trainLoss:0.1875387206673622
epoch:8 trainLoss:0.18141555041074753
epoch:9 trainLoss:0.17621422559022903
epoch:10 trainLoss:0.17187954485416412
TESTING: epoch:10 testLoss:0.20497536659240723
epoch:11 trainLoss:0.16823899745941162
epoch:12 trainLoss:0.1651960164308548
epoch:13 trainLoss:0.16249831765890121
epoch:14 trainLoss:0.16004575788974762
epoch:15 trainLoss:0.15783895552158356
epoch:16 trainLoss:0.15590006858110428
epoch:17 trainLoss:0.15417761355638504
epoch:18 trainLoss:0.1526780128479004
epoch:19 trainLoss:0.15129715204238892
epoch:20 trainLoss:0.14998625218868256
TESTING: epoch:20 testLoss:0.19143331050872803
epoch:21 trainLoss:0.14877121150493622
epoch:22 trainLoss:0.14761385321617126
epoch:23 trainLoss:0.1464502289891243
epoch:24 trainLoss:0.14529041945934296
epoch:25 trainLoss:0.1441396325826645
epoch:26 trainLoss:0.14300644397735596
epoch:27 trainLoss:0.14189603179693222
epoch:28 trainLoss:0.14079886674880981
epoch:29 trainLoss:0.1397378295660019
epoch:30 trainLoss:0.1386910304427147
TESTING: epoch:30 testLoss:0.19344906508922577
epoch:31 trainLoss:0.13766015321016312
epoch:32 trainLoss:0.1366293951869011
epoch:33 trainLoss:0.1356128454208374
epoch:34 trainLoss:0.13459813594818115
epoch:35 trainLoss:0.1335820108652115
epoch:36 trainLoss:0.13257208466529846
epoch:37 trainLoss:0.1315579004585743
epoch:38 trainLoss:0.13054485991597176
epoch:39 trainLoss:0.12954340502619743
epoch:40 trainLoss:0.12852896749973297
TESTING: epoch:40 testLoss:0.19639332592487335
epoch:41 trainLoss:0.12753695622086525
epoch:42 trainLoss:0.1265474259853363
epoch:43 trainLoss:0.12554101645946503
epoch:44 trainLoss:0.12454494088888168
epoch:45 trainLoss:0.12355882674455643
epoch:46 trainLoss:0.12255525216460228
epoch:47 trainLoss:0.12155519798398018
epoch:48 trainLoss:0.12057588249444962
epoch:49 trainLoss:0.11959801241755486
epoch:50 trainLoss:0.11861918866634369
TESTING: epoch:50 testLoss:0.20095640420913696
epoch:51 trainLoss:0.11764708906412125
epoch:52 trainLoss:0.11666914448142052
epoch:53 trainLoss:0.11569315195083618
epoch:54 trainLoss:0.11473176628351212
epoch:55 trainLoss:0.11377235129475594
epoch:56 trainLoss:0.11282655969262123
epoch:57 trainLoss:0.11187063157558441
epoch:58 trainLoss:0.11091655120253563
epoch:59 trainLoss:0.10996731743216515
epoch:60 trainLoss:0.10901818051934242
TESTING: epoch:60 testLoss:0.20493434369564056
epoch:61 trainLoss:0.10806415602564812
epoch:62 trainLoss:0.10711148008704185
epoch:63 trainLoss:0.10616165772080421
epoch:64 trainLoss:0.10521512478590012
epoch:65 trainLoss:0.10428672656416893
epoch:66 trainLoss:0.10335924848914146
epoch:67 trainLoss:0.10243434086441994
epoch:68 trainLoss:0.10151709243655205
epoch:69 trainLoss:0.10060818120837212
epoch:70 trainLoss:0.09970807656645775
TESTING: epoch:70 testLoss:0.20804163813591003
epoch:71 trainLoss:0.09881740808486938
epoch:72 trainLoss:0.09792060405015945
epoch:73 trainLoss:0.09703364595770836
epoch:74 trainLoss:0.09615419432520866
epoch:75 trainLoss:0.09528231993317604
epoch:76 trainLoss:0.09441160783171654
epoch:77 trainLoss:0.09355365112423897
epoch:78 trainLoss:0.09270605444908142
epoch:79 trainLoss:0.0918804220855236
epoch:80 trainLoss:0.09104244038462639
TESTING: epoch:80 testLoss:0.21136239171028137
epoch:81 trainLoss:0.09022875502705574
epoch:82 trainLoss:0.08941170573234558
epoch:83 trainLoss:0.08862806484103203
epoch:84 trainLoss:0.0878266803920269
epoch:85 trainLoss:0.08704304695129395
epoch:86 trainLoss:0.08628852292895317
epoch:87 trainLoss:0.08552044630050659
epoch:88 trainLoss:0.08478184789419174
epoch:89 trainLoss:0.08403068780899048
epoch:90 trainLoss:0.08329955860972404
TESTING: epoch:90 testLoss:0.21440890431404114
epoch:91 trainLoss:0.08257080614566803
epoch:92 trainLoss:0.08185870200395584
epoch:93 trainLoss:0.08115062490105629
epoch:94 trainLoss:0.08046628907322884
epoch:95 trainLoss:0.07976619526743889
epoch:96 trainLoss:0.07907590642571449
epoch:97 trainLoss:0.07841348275542259
epoch:98 trainLoss:0.07772480323910713
epoch:99 trainLoss:0.07706866413354874
